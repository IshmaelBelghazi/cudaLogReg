% Generated by roxygen2 (4.1.0.9001): do not edit by hand
% Please edit documentation in R/generics.R
\name{get_grad}
\alias{get_grad}
\alias{get_grad.model.spec}
\title{Computes model gradient}
\usage{
get_grad(object, feats, targets, decay = NULL, backend = "R", ...)

\method{get_grad}{model.spec}(object, feats, targets, decay = NULL,
  backend = "R", ...)
}
\arguments{
\item{object}{Linear classifier specification object.}

\item{feats}{Numeric Matrix of features. Follows the usual convention of having
one example per row. For a model with M features and dataset with N
examples the matrix should be N \eqn{\times} K}

\item{targets}{Numeric matrix of one hot encoded target. Follows the usual
convention of having one target per row. For a model with K classes and a
dataset with N examples the matrix should be N \eqn{\times} K}

\item{decay}{Numeric scalar. Tikhonov regularization coefficient (weight decay). Should be
a non-negative real number.}

\item{backend}{Computation back-end ('R', 'C', or 'CUDA')}

\item{...}{other arguments passed to specific methods}
}
\value{
Numeric Matrix of gradients. One for each class. Gradient are arrayed in
columns. For a model with M features and K classes the matrix should be M
\eqn{\times} K
}
\description{
Computes model gradient
}
\section{Methods (by class)}{
\itemize{
\item \code{model.spec}: Computes model gradient for linear classifier
specification objects
}}
\examples{
# Generate random initial weights
w_init <- matrix(rnorm(784 * 10), 784, 10)
# construct model
linear_classifier <- Classifier(weights_init=w_init)
# Fetch training variables
feats <- mini_mnist$train$images
targets <- mini_mnist$train$labels
# Set decay coefficient
decay <- 0.01
# compute gradient at the training set using the three back-ends
gradient_R <- get_grad(linear_classifier, feats, targets, decay, 'R')
gradient_C <- get_grad(linear_classifier, feats, targets, decay, 'C')
}
\author{
Mohamed Ishmael Diwan Belghazi
}

